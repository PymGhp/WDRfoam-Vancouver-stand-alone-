Unloading StdEnv/2020

The following have been reloaded with a version change:
  1) gcccore/.9.3.0 => gcccore/.7.3.0     3) openmpi/4.0.3 => openmpi/3.1.4
  2) mii/1.1.2 => mii/1.1.1

/cvmfs/soft.computecanada.ca/easybuild/software/2017/avx512/MPI/gcc7.3/openmpi3.1/openfoam/7/OpenFOAM-7/etc/config.sh/paraview: line 102: [: too many arguments
/*---------------------------------------------------------------------------*\
  =========                 |
  \\      /  F ield         | OpenFOAM: The Open Source CFD Toolbox
   \\    /   O peration     | Website:  https://openfoam.org
    \\  /    A nd           | Version:  7
     \\/     M anipulation  |
\*---------------------------------------------------------------------------*/
Build  : 7
Exec   : decomposePar -force
Date   : Aug 04 2023
Time   : 05:05:38
Host   : "bc12336.int.ets1.calculquebec.ca"
PID    : 248142
I/O    : uncollated
Case   : /lustre04/scratch/payamghp/cubicBuilding.simpleFoam_kOmega_overhang12
nProcs : 1
sigFpe : Enabling floating point exception trapping (FOAM_SIGFPE).
fileModificationChecking : Monitoring run-time modified files using timeStampMaster (fileModificationSkew 10)
allowSystemOperations : Allowing user-supplied system call operations

// * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * //
Create time

Removing 0 existing processor directories


Decomposing mesh region0

Create mesh

Calculating distribution of cells
 Foam::decompositionMethod::New 
{
    numberOfSubdomains 8;
    method          simple;
    simpleCoeffs
    {
        n               ( 4 1 2 );
        delta           0.001;
    }
    hierarchicalCoeffs
    {
        n               ( 2 3 1 );
        delta           0.001;
        order           xyz;
    }
    manualCoeffs
    {
        dataFile        "cellDecomposition";
    }
}

Selecting decompositionMethod simple

Finished decomposition in 4.28 s

Calculating original mesh data

Distributing cells to processors

Distributing faces to processors

Distributing points to processors

Constructing processor meshes

Processor 0
    Number of cells = 658084
    Number of faces shared with processor 1 = 14099
    Number of faces shared with processor 4 = 5804
    Number of faces shared with processor 5 = 1
    Number of processor patches = 3
    Number of processor faces = 19904
    Number of boundary faces = 30566

Processor 1
    Number of cells = 655451
    Number of faces shared with processor 0 = 14099
    Number of faces shared with processor 2 = 11260
    Number of faces shared with processor 5 = 4841
    Number of processor patches = 3
    Number of processor faces = 30200
    Number of boundary faces = 23014

Processor 2
    Number of cells = 653369
    Number of faces shared with processor 1 = 11260
    Number of faces shared with processor 3 = 13852
    Number of faces shared with processor 5 = 1
    Number of faces shared with processor 6 = 5258
    Number of faces shared with processor 7 = 7
    Number of processor patches = 5
    Number of processor faces = 30378
    Number of boundary faces = 21788

Processor 3
    Number of cells = 647716
    Number of faces shared with processor 2 = 13852
    Number of faces shared with processor 7 = 5947
    Number of processor patches = 2
    Number of processor faces = 19799
    Number of boundary faces = 30017

Processor 4
    Number of cells = 649226
    Number of faces shared with processor 0 = 5804
    Number of faces shared with processor 5 = 13915
    Number of processor patches = 2
    Number of processor faces = 19719
    Number of boundary faces = 30283

Processor 5
    Number of cells = 651859
    Number of faces shared with processor 0 = 1
    Number of faces shared with processor 1 = 4841
    Number of faces shared with processor 2 = 1
    Number of faces shared with processor 4 = 13915
    Number of faces shared with processor 6 = 11423
    Number of processor patches = 5
    Number of processor faces = 30181
    Number of boundary faces = 23179

Processor 6
    Number of cells = 653941
    Number of faces shared with processor 2 = 5258
    Number of faces shared with processor 5 = 11423
    Number of faces shared with processor 7 = 14069
    Number of processor patches = 3
    Number of processor faces = 30750
    Number of boundary faces = 21462

Processor 7
    Number of cells = 659594
    Number of faces shared with processor 2 = 7
    Number of faces shared with processor 3 = 5947
    Number of faces shared with processor 6 = 14069
    Number of processor patches = 3
    Number of processor faces = 20023
    Number of boundary faces = 30717

Number of processor faces = 100477
Max number of cells = 659594 (0.908583% above average 653655)
Max number of processor patches = 5 (53.8462% above average 3.25)
Max number of faces between processors = 30750 (22.4161% above average 25119.2)

Time = 0


--> FOAM FATAL IO ERROR: 
inconsistent patch and patchField types for 
    patch type symmetry and patchField type slip

file: /lustre04/scratch/payamghp/cubicBuilding.simpleFoam_kOmega_overhang12/0/epsilon.boundaryField.top from line 27 to line 27.

    From function static Foam::tmp<Foam::fvPatchField<Type> > Foam::fvPatchField<Type>::New(const Foam::fvPatch&, const Foam::DimensionedField<Type, Foam::volMesh>&, const Foam::dictionary&) [with Type = double]
    in file /cvmfs/soft.computecanada.ca/easybuild/software/2017/avx512/MPI/gcc7.3/openmpi3.1/openfoam/7/OpenFOAM-7/src/finiteVolume/lnInclude/fvPatchFieldNew.C at line 160.

FOAM exiting

/*---------------------------------------------------------------------------*\
  =========                 |
  \\      /  F ield         | OpenFOAM: The Open Source CFD Toolbox
   \\    /   O peration     | Website:  https://openfoam.org
    \\  /    A nd           | Version:  7
     \\/     M anipulation  |
\*---------------------------------------------------------------------------*/
Build  : 7
Exec   : /cvmfs/soft.computecanada.ca/easybuild/software/2017/avx512/MPI/gcc7.3/openmpi3.1/openfoam/7/OpenFOAM-7/platforms/linux64GccDPInt32Opt/bin/simpleFoam -parallel
Date   : Aug 04 2023
Time   : 05:08:52
Host   : "bc12336.int.ets1.calculquebec.ca"
PID    : 54090
I/O    : uncollated
Case   : /lustre04/scratch/payamghp/cubicBuilding.simpleFoam_kOmega_overhang12
nProcs : 8
Slaves : 
7
(
"bc12336.int.ets1.calculquebec.ca.54095"
"bc12336.int.ets1.calculquebec.ca.54096"
"bc12336.int.ets1.calculquebec.ca.54097"
"bc12336.int.ets1.calculquebec.ca.54098"
"bc12336.int.ets1.calculquebec.ca.54099"
"bc12336.int.ets1.calculquebec.ca.54110"
"bc12336.int.ets1.calculquebec.ca.54116"
)

Pstream initialized with:
    floatTransfer      : 0
    nProcsSimpleSum    : 0
    commsType          : nonBlocking
    polling iterations : 0
sigFpe : Enabling floating point exception trapping (FOAM_SIGFPE).
fileModificationChecking : Monitoring run-time modified files using timeStampMaster (fileModificationSkew 10)
allowSystemOperations : Allowing user-supplied system call operations

// * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * //
Create time

Create mesh for time = 0


SIMPLE: Convergence criteria found
        p: tolerance 1e-05
        U: tolerance 1e-05
        "(k|epsilon|omega)": tolerance 1e-05

Reading field p

[7] 
[7] 
[7] --> FOAM FATAL ERROR: 
[0] 
[0] 
[0] --> FOAM FATAL ERROR: 
[0] cannot find file "/lustre04/scratch/payamghp/cubicBuilding.simpleFoam_kOmega_overhang12/processor0/0/p"
[0] 
[0]     From function virtual Foam::autoPtr<Foam::ISstream> Foam::fileOperations::uncollatedFileOperation::readStream(Foam::regIOobject&, const Foam::fileName&, const Foam::word&, bool) const
[7] cannot find file "/lustre04/scratch/payamghp/cubicBuilding.simpleFoam_kOmega_overhang12/processor7/0/p"
[7] 
[7]     From function virtual Foam::autoPtr<Foam::ISstream> Foam::fileOperations::uncollatedFileOperation::readStream(Foam::regIOobject&, const Foam::fileName&, const Foam::word&, bool) const
[7]     in file global/fileOperations/uncollatedFileOperation/uncollatedFileOperation.C at line 538.
[7] 
FOAM parallel run exiting
[7] 
[0]     in file global/fileOperations/uncollatedFileOperation/uncollatedFileOperation.C at line 538.
[0] 
FOAM parallel run exiting
[0] 
[6] 
[6] 
[6] --> FOAM FATAL ERROR: 
[6] cannot find file "/lustre04/scratch/payamghp/cubicBuilding.simpleFoam_kOmega_overhang12/processor6/0/p"
[6] 
[6]     From function virtual Foam::autoPtr<Foam::ISstream> Foam::fileOperations::uncollatedFileOperation::readStream(Foam::regIOobject&, const Foam::fileName&, const Foam::word&, bool) const
[6]     in file global/fileOperations/uncollatedFileOperation/uncollatedFileOperation.C at line 538.
[6] 
FOAM parallel run exiting
[6] 
[2] 
[5] 
[5] 
[2] 
[2] --> FOAM FATAL ERROR: 
[5] --> FOAM FATAL ERROR: 
[5] cannot find file "/lustre04/scratch/payamghp/cubicBuilding.simpleFoam_kOmega_overhang12/processor5/0/p"
[5] 
[2] cannot find file "/lustre04/scratch/payamghp/cubicBuilding.simpleFoam_kOmega_overhang12/processor2/0/p"
[2] 
[2]     From function virtual Foam::autoPtr<Foam::ISstream> Foam::fileOperations::uncollatedFileOperation::readStream(Foam::regIOobject&, const Foam::fileName&, const Foam::word&, bool) const
[5]     From function virtual Foam::autoPtr<Foam::ISstream> Foam::fileOperations::uncollatedFileOperation::readStream(Foam::regIOobject&, const Foam::fileName&, const Foam::word&, bool) const
[5]     in file global/fileOperations/uncollatedFileOperation/uncollatedFileOperation.C at line 538.
[5] 
FOAM parallel run exiting
[5] 
[2]     in file global/fileOperations/uncollatedFileOperation/uncollatedFileOperation.C at line 538.
[2] 
FOAM parallel run exiting
[2] 
[4] 
[4] 
[4] --> FOAM FATAL ERROR: 
[4] cannot find file "/lustre04/scratch/payamghp/cubicBuilding.simpleFoam_kOmega_overhang12/processor4/0/p"
[4] 
[4]     From function virtual Foam::autoPtr<Foam::ISstream> Foam::fileOperations::uncollatedFileOperation::readStream(Foam::regIOobject&, const Foam::fileName&, const Foam::word&, bool) const
[4]     in file global/fileOperations/uncollatedFileOperation/uncollatedFileOperation.C at line 538.
[4] 
FOAM parallel run exiting
[4] 
[3] 
[3] 
[3] --> FOAM FATAL ERROR: 
[3] cannot find file "/lustre04/scratch/payamghp/cubicBuilding.simpleFoam_kOmega_overhang12/processor3/0/p"
[3] 
[3]     From function virtual Foam::autoPtr<Foam::ISstream> Foam::fileOperations::uncollatedFileOperation::readStream(Foam::regIOobject&, const Foam::fileName&, const Foam::word&, bool) const
[3]     in file global/fileOperations/uncollatedFileOperation/uncollatedFileOperation.C at line 538.
[3] 
FOAM parallel run exiting
[3] 
[1] 
[1] 
[1] --> FOAM FATAL ERROR: 
[1] cannot find file "/lustre04/scratch/payamghp/cubicBuilding.simpleFoam_kOmega_overhang12/processor1/0/p"
[1] 
[1]     From function virtual Foam::autoPtr<Foam::ISstream> Foam::fileOperations::uncollatedFileOperation::readStream(Foam::regIOobject&, const Foam::fileName&, const Foam::word&, bool) const
[1]     in file global/fileOperations/uncollatedFileOperation/uncollatedFileOperation.C at line 538.
[1] 
FOAM parallel run exiting
[1] 
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI COMMUNICATOR 3 SPLIT FROM 0
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 1 in communicator MPI COMMUNICATOR 3 SPLIT FROM 0
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 2 in communicator MPI COMMUNICATOR 3 SPLIT FROM 0
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 3 in communicator MPI COMMUNICATOR 3 SPLIT FROM 0
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 4 in communicator MPI COMMUNICATOR 3 SPLIT FROM 0
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 5 in communicator MPI COMMUNICATOR 3 SPLIT FROM 0
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 6 in communicator MPI COMMUNICATOR 3 SPLIT FROM 0
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 7 in communicator MPI COMMUNICATOR 3 SPLIT FROM 0
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
slurmstepd: error: *** STEP 39551636.0 ON bc12336 CANCELLED AT 2023-08-04T09:08:59 ***
srun: Job step aborted: Waiting up to 62 seconds for job step to finish.
srun: error: bc12336: tasks 0-7: Killed
